{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87e670d-7fe5-4433-9987-d2d99739ef5c",
   "metadata": {
    "id": "f87e670d-7fe5-4433-9987-d2d99739ef5c"
   },
   "source": [
    "# DT-BASE\n",
    "#### Causal Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4824d6-c30c-43b6-a639-0a731e953f3a",
   "metadata": {
    "id": "6b4824d6-c30c-43b6-a639-0a731e953f3a"
   },
   "outputs": [],
   "source": [
    "__author__ = 'Andrea Roy (SoTeRiA Research Laboratory)'\n",
    "__version__ = '1.0'\n",
    "__email__ = 'akroy2@illinois.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a541a-981a-41bd-b3b6-8ff4e3f65a59",
   "metadata": {
    "id": "316a541a-981a-41bd-b3b6-8ff4e3f65a59"
   },
   "source": [
    "## Introduction\n",
    "This notebook builds and executes the DT-BASE causal model case study for Training Quality presented in [this](https://github.com/SoTeRiALab/dt-base.git) publication using the updated methodology developed as part of the PSA 2021 conference paper \"Managing Unexpected Failures of Nuclear Power Plants by Quantifying Uncertainty of Organizational Impacts on Probabilistic Risk Assessment\" (accepted Aug. 2021). \n",
    "\n",
    "#### What is DT?\n",
    "`DT-BASE` is a submodule of the **Data-Theoretic (DT)** approach to quantifying organizational impacts on risk scenarios. DT combines theoretical causal modeling of organizational factors with plant-specific data to provide enhanced realism in the quantification of organizational factors' impacts on risk scenarios, and avoid potentially misleading conclusions which are possible in solely data-oriented approaches.\n",
    "\n",
    "#### Bayesian Belief Network\n",
    "`DT-BASE` leverages a Bayesian Belief Network (BBN) to model the extended causality between organizational factors. A BBN is a statistical learning technique which uses a graphical network in the form of a Directed Acyclic Graph (DAG). Each node in the DT-BASE model represents an organizational factor (e.g., training procedure quality, program design quality, etc.). For simplicity, each organizational factor is rated on a scale of `{Good, Bad}` (the \"states\" of each node). Each edge represents a *causal relationship* between organizational facotrs (e.g., The goal is to quantify `P(Target = Good)`, where `Target` is the \"target node,\" or the \"unknown of interest\" (e.g., training quality). For every node (organizational factor) in the network, a conditional probability table (CPT) must be developed to model the conditional probability of each child node being in a `Good` or `Poor` stage given the state of a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b6b35-878d-4a95-aec2-759f195d9f73",
   "metadata": {
    "id": "f93b6b35-878d-4a95-aec2-759f195d9f73"
   },
   "source": [
    "#### Importing Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30de5a2-37aa-48fa-8589-d37a9c1300aa",
   "metadata": {
    "id": "b30de5a2-37aa-48fa-8589-d37a9c1300aa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41e86b-23c9-4d67-bfda-834123c7d25a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "4e41e86b-23c9-4d67-bfda-834123c7d25a",
    "outputId": "9c02d6e8-83e0-4fdc-b9c1-b95de101b066"
   },
   "outputs": [],
   "source": [
    "# import references into a pandas dataframe.\n",
    "refs = pd.read_csv('data/refs.csv')\n",
    "refs.set_index('ref_id', inplace=True)\n",
    "refs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7cdda-d112-4d46-95ab-4e01ad42deb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "c6b7cdda-d112-4d46-95ab-4e01ad42deb0",
    "outputId": "29a23006-79f7-4ec8-a3f7-da1d3f14dbd5"
   },
   "outputs": [],
   "source": [
    "# import nodes into a pandas dataframe\n",
    "nodes = pd.read_csv('data/nodes.csv')\n",
    "nodes.set_index('node_id', inplace=True)\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86caba51-2a68-4101-a903-15531115272b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "86caba51-2a68-4101-a903-15531115272b",
    "outputId": "f15d327d-410c-493c-881e-b380593774e0"
   },
   "outputs": [],
   "source": [
    "# import edge data into a pandas dataframe\n",
    "edges = pd.read_csv('data/edges.csv')\n",
    "edges.set_index('edge_id', inplace=True)\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60855ed8-1a6c-4556-b0c1-04a9cc1bb0cb",
   "metadata": {
    "id": "60855ed8-1a6c-4556-b0c1-04a9cc1bb0cb"
   },
   "source": [
    "#### Uncertainty Characterization\n",
    "There is some element of subjectivity involved in the analyst's estimation of the m1, m2, and m3 values shown, which represent the analyst's judgment of the strength of the causal relationship between parent and child nodes. We model this uncertainty in the analyst's interpretation using an upper and lower bound of the estimate values, rather than simply a point estimate. This allows us to propagate the uncertainty in the analyst's estimates to later stages of the model, using [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ad8a5-24ee-4fe9-9fd3-09996524b217",
   "metadata": {
    "id": "953ad8a5-24ee-4fe9-9fd3-09996524b217"
   },
   "outputs": [],
   "source": [
    "numSamples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02047323-fcf7-4a41-9e90-92a9e51e0f1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "02047323-fcf7-4a41-9e90-92a9e51e0f1a",
    "outputId": "bd413d0e-538f-4e09-ecf9-166ec7152d4a"
   },
   "outputs": [],
   "source": [
    "m1 = pd.DataFrame(columns=refs.index)\n",
    "m2 = pd.DataFrame(columns=edges.index)\n",
    "m3 = pd.DataFrame(columns=edges.index)\n",
    "# generate samples from a uniform distribution for m1, m2, and m3\n",
    "for ref_id in refs.index:\n",
    "    m1[ref_id] = np.random.uniform(refs['m1A'][ref_id], refs['m1B'][ref_id], size=(numSamples, ))\n",
    "    \n",
    "for edge_id in edges.index:\n",
    "    m2[edge_id] = np.random.uniform(edges['m2A'][edge_id], edges['m2B'][edge_id], size=(numSamples, ))\n",
    "    m3[edge_id] = np.random.uniform(edges['m3A'][edge_id], edges['m3B'][edge_id], size=(numSamples, ))\n",
    "\n",
    "m1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa085586-0a0f-4169-8449-c870b3afd3d1",
   "metadata": {
    "id": "aa085586-0a0f-4169-8449-c870b3afd3d1"
   },
   "source": [
    "#### Aggregating Edges\n",
    "The DT-BASE methodology suggests that a minimum of 3 sources of evidence should be collected to support each causal relationship (edge) in the model. As a result, we need a way to aggregate multiple sources of information into a single edge \"weight.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246de73-69dd-4bb1-ba28-237f40ada270",
   "metadata": {
    "id": "2246de73-69dd-4bb1-ba28-237f40ada270"
   },
   "outputs": [],
   "source": [
    "# calculate the \"normalized\" weight of every edge in the graph using arithmetic mean\n",
    "normWeights = {}\n",
    "for edge_id in edges.index:\n",
    "    edge = (edges.parent_id[edge_id], edges.child_id[edge_id])\n",
    "    normWeights[edge] = normWeights.get(edge, 0) + m1[edges.ref_id[edge_id]] * m3[edge_id]\n",
    "    \n",
    "weights = {}\n",
    "for edge_id in edges.index:\n",
    "    edge = (edges.parent_id[edge_id], edges.child_id[edge_id])\n",
    "    weights[edge_id] = (m1[edges.ref_id[edge_id]] * m3[edge_id]) / normWeights[edge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5c3878-9a23-4786-96b6-00bbcc13c56c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e5c3878-9a23-4786-96b6-00bbcc13c56c",
    "outputId": "70926303-0475-4dca-d289-a56ea392921a"
   },
   "outputs": [],
   "source": [
    "# create superEdges (edges which are the aggregation of multiple edges supporting the same causal relationship)\n",
    "superEdges = {}\n",
    "for edge_id in edges.index:\n",
    "    edge = (edges.parent_id[edge_id], edges.child_id[edge_id])\n",
    "    superEdges[edge] = superEdges.get(edge, 0) + weights[edge_id] * m2[edge_id]\n",
    "superEdges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5f520-8e0e-4a92-8770-de05c7f63206",
   "metadata": {
    "id": "b6c5f520-8e0e-4a92-8770-de05c7f63206"
   },
   "source": [
    "#### DT-BASE BBN\n",
    "\n",
    "We now have all the information we need to build the DT-BASE BBN. Let's plot a visual representation of the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de756e9-954b-4e6c-80bf-d446a7970720",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "3de756e9-954b-4e6c-80bf-d446a7970720",
    "outputId": "b3326799-f156-42fe-9a8e-1d0fdcb780fb"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pylab import draw_networkx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(superEdges)\n",
    "draw_networkx(G, pos=nx.spring_layout(G), arrowsize=20, node_size=1200, node_color=\"#DDDDDD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb83f5-f309-43f5-afff-3474714ae4b5",
   "metadata": {
    "id": "a3fb83f5-f309-43f5-afff-3474714ae4b5"
   },
   "source": [
    "#### Calculate Conditional Probability Tables (CPTs)\n",
    "The next step in the execution of the model is to calculate conditional probability tables (CPTs). For this calculation, we use the Noisy-Or approximation. A discussion of this method is beyond the scope of this activity, but if you are interested, we recommend you read [this](https://people.csail.mit.edu/dsontag/papers/HalpernSontag_uai13.pdf) paper for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe21052-1c50-46de-b5fc-72743ff5bef5",
   "metadata": {
    "id": "7fe21052-1c50-46de-b5fc-72743ff5bef5"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pyeda\n",
    "from pyeda.inter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c8f2a-3ae1-4468-a4be-63127b3a1c8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "095c8f2a-3ae1-4468-a4be-63127b3a1c8a",
    "outputId": "e83acae8-b467-405e-da5f-65166ca0960e"
   },
   "outputs": [],
   "source": [
    "# develop CPT combinations for each node\n",
    "cptCombos = {}\n",
    "for adj in G.reverse().adjacency():\n",
    "    node = adj[0]\n",
    "    incoming = list(adj[1].keys())\n",
    "    bools = []\n",
    "    tmp = itertools.product([True, False], repeat = len(incoming))\n",
    "    for t in tmp:\n",
    "        bools.append(tuple([exprvar(incoming[i]) if t[i] else ~exprvar(incoming[i]) for i in range(len(t))]))\n",
    "    cptCombos[node] = bools\n",
    "cptCombos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887905d-db21-4b22-b780-0a4767c49cd4",
   "metadata": {
    "id": "1887905d-db21-4b22-b780-0a4767c49cd4"
   },
   "outputs": [],
   "source": [
    "# create a leak variable, representing the probability that the model is incomplete.\n",
    "leak = np.random.uniform(.9, .99, (numSamples, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cc729-1bee-4448-8f74-f1f37fa2adbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e51cc729-1bee-4448-8f74-f1f37fa2adbe",
    "outputId": "96b73b10-0929-4c5c-fe5b-9b4ad4512a67"
   },
   "outputs": [],
   "source": [
    "# calculate the conditional probability table for each node, using the CPT combinations, and the noisy-or approximation.\n",
    "cpts = { node: [] for node in nodes.index }\n",
    "for node, combinations in cptCombos.items():\n",
    "    for combo in combinations:\n",
    "        tmp = 1\n",
    "        for parent in combo:\n",
    "            if isinstance(parent, pyeda.boolalg.expr.Variable):\n",
    "                tmp *= (1 - superEdges[parent.name, node])\n",
    "            cpts[node].extend([1 - leak * tmp, leak * tmp])\n",
    "            \n",
    "# print out the CPT for each node\n",
    "{ node: [np.mean(cptVec) for cptVec in cpt] for node, cpt in cpts.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8058b7-4c77-42a0-8656-47cf19e2ca59",
   "metadata": {
    "id": "8a8058b7-4c77-42a0-8656-47cf19e2ca59"
   },
   "source": [
    "#### BBN Quantification\n",
    "A BBN model consists of two main features: (1) a DAG representing the structure of the model, and (2) a set of CPTs, one for each node in the model. Now that we have both of these aspects of the BBN model, we can quantify the model. To do this, we will use the `pybbn` python package for Bayesian Inference. We will calculate the `P(tq=Bad)` using the inference algorithm provided by `pybbn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78265f50-89df-419e-a3de-16acd7e8ef45",
   "metadata": {
    "id": "78265f50-89df-419e-a3de-16acd7e8ef45"
   },
   "outputs": [],
   "source": [
    "from pybbn.graph.dag import Bbn\n",
    "from pybbn.graph.edge import Edge, EdgeType\n",
    "from pybbn.graph.jointree import EvidenceBuilder\n",
    "from pybbn.graph.node import BbnNode\n",
    "from pybbn.graph.variable import Variable\n",
    "from pybbn.pptc.inferencecontroller import InferenceController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1MuVW_WkMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c1MuVW_WkMZ",
    "outputId": "dddc0b59-e94a-4374-dcee-ea299fde5006"
   },
   "outputs": [],
   "source": [
    "# create BBN nodes\n",
    "bbnNodes = { node.Index : BbnNode(Variable(i, node.Index, ['Good', 'Bad']), [1, 0]) for i, node in enumerate(nodes.itertuples()) }\n",
    "bbnNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aE5tUOyRZKcu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aE5tUOyRZKcu",
    "outputId": "d8077ac9-58c0-417e-a77f-57bbea6dd3de"
   },
   "outputs": [],
   "source": [
    "# create BBN edges\n",
    "bbnEdges = [ Edge(a, b, EdgeType.DIRECTED) for a, b in superEdges]\n",
    "bbnEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqNwOnw3ZcMF",
   "metadata": {
    "id": "sqNwOnw3ZcMF"
   },
   "outputs": [],
   "source": [
    "# construct the DT-BASE BBN graph using the nodes and edges above.\n",
    "dt_base = Bbn()\n",
    "for node in bbnNodes.values():\n",
    "  dt_base.add_node(node)\n",
    "for edge in bbnEdges:\n",
    "  dt_base.add_edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lg3ylR85c0Re",
   "metadata": {
    "id": "Lg3ylR85c0Re"
   },
   "source": [
    "Now, we can run the BBN inference algorithm in the cell below. Please note that this cell may take a couple minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TVTA2rqhZ0jl",
   "metadata": {
    "id": "TVTA2rqhZ0jl"
   },
   "outputs": [],
   "source": [
    "# perform Bayesian inference for the target node \"tq\"\n",
    "# run the model numSamples times to obtain a probability distribution\n",
    "target = bbnNodes['tq']\n",
    "target_probs = []\n",
    "\n",
    "for sample in range(numSamples):\n",
    "  for node_id, node in bbnNodes.items():\n",
    "    probs = [p[sample] for p in cpts[node_id]] if cpts[node_id] else [1, 0]\n",
    "    node.probs = probs\n",
    "    \n",
    "  join_tree = InferenceController.apply(dt_base)\n",
    "  target_probs.append(join_tree.get_bbn_potential(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flEAAHF_e4Fh",
   "metadata": {
    "id": "flEAAHF_e4Fh"
   },
   "source": [
    "#### Analysis\n",
    "Now that we have calculated the probability distribution for the target node \"Training Quality\" being in a \"Bad\" state, let us display our results and do some simple data analysis.\n",
    "\n",
    "First, let's output the raw probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8jLZwqfPSX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b8jLZwqfPSX",
    "outputId": "a02c9a2d-1513-4b20-878e-33598f5f99d1"
   },
   "outputs": [],
   "source": [
    "p = pd.Series([t.entries[1].value for t in target_probs])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197r9LJOfsjX",
   "metadata": {
    "id": "197r9LJOfsjX"
   },
   "source": [
    "Now, let's output the summary statistics for the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MiG3tSSkf0fJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MiG3tSSkf0fJ",
    "outputId": "bd4de1bd-e914-450b-fe76-b6a07cea869d"
   },
   "outputs": [],
   "source": [
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eN_-rlvtgIK3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "eN_-rlvtgIK3",
    "outputId": "50c6a65d-6a43-4c7f-c266-fe4e9e1c4871"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.hist(p, 100)\n",
    "ax.set_title('DT-BASE Baseline Quantification')\n",
    "ax.set_xlabel('P(Training Quality = \"Bad\")')\n",
    "ax.set_ylabel('PDF')\n",
    "plt.style.use('seaborn')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dt-base.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
